{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BertMultilingual_NER.ipynb","provenance":[],"mount_file_id":"1ouN0oEcj8NpHrJIv3g2uZSXv9pli_wgJ","authorship_tag":"ABX9TyOr7CeBBlwdam+tc5RHUTOU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"I6wtdTPTo316","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":212},"executionInfo":{"status":"error","timestamp":1596376621946,"user_tz":-420,"elapsed":2868,"user":{"displayName":"Thang Nguyen Van","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6slKndYJSgUMFaF-5nhOx6wo53oKdQ_HFjoZYew=s64","userId":"01415845043637983403"}},"outputId":"06364003-b320-42f7-c5ac-07342e8814c5"},"source":["import tensorflow as tf\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SystemError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a532c2b9b173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSystemError\u001b[0m: GPU device not found"]}]},{"cell_type":"code","metadata":{"id":"PAHH2POBS33Z","colab_type":"code","colab":{}},"source":["!pip install pytorch-pretrained-bert pytorch-nlp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tskf4dGMaXui","colab_type":"code","colab":{}},"source":["!pip install transformers "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_xeR6ywS4Ad","colab_type":"code","colab":{}},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer\n","from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import StratifiedKFold\n","% matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byXooJCES4GV","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IRXfYoXTS4JV","colab_type":"code","colab":{}},"source":["data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/NLP/data/DataNer.txt\", sep=\" \", header=None, skip_blank_lines=False, encoding=\"utf-8\")\n","data.columns = [\"word\",\"pos\", \"upper\", \"number\",\"tag\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"38Rcj_HgS4MV","colab_type":"code","colab":{}},"source":["print(df.intent.unique())\n","intents = df.intent.unique()\n","print(\"Total categories\",len(df.intent.unique()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGI3hEYKS4PO","colab_type":"code","colab":{}},"source":["sentences_with_tag = []\n","sentence_with_tag = []\n","for w,t in zip(data['word'].values, data['tag'].values):\n","  if isNaN(w)==False:\n","    sentence_with_tag.append((w,t))\n","  if isNaN(w)==True: \n","    sentences_with_tag.append(sentence_with_tag)\n","    sentence_with_tag = []\n","sentences_with_tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jlpEKYp5cZgv","colab_type":"code","colab":{}},"source":["sentences = [' '.join([word[0] for word in sent]) for sent in sentences_with_tag]\n","sentences[2593]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WcRwZm-JcZrp","colab_type":"code","colab":{}},"source":["labels = [[word[1] for word in sent] for sent in sentences_with_tag]\n","labels[2]\n","# labels = labels "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yzjWo6mvceWa","colab_type":"code","colab":{}},"source":["classes = list(filter(lambda x: x not in [\"O\", np.nan], list(data[\"tag\"].unique())))\n","classes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_mdK26Cceaj","colab_type":"code","colab":{}},"source":["labels_value = classes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sn2vIkzLcmIs","colab_type":"code","colab":{}},"source":["labels_value.append('O')\n","# intents = labels_value"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQdcoua5cmOR","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zvv-ac8yS4Rr","colab_type":"code","colab":{}},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","print (\"Tokenize the first sentence:\")\n","print (tokenized_texts[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"moxnfzdHS4UV","colab_type":"code","colab":{}},"source":["max_len = 0\n","# For every sentence...\n","for sent in sentences:\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","    # Update the maximum sentence length.\n","    max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQHyVO1XS4XF","colab_type":"code","colab":{}},"source":["MAX_LEN = 256"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbuAhMt2vDR3","colab_type":"code","colab":{}},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = MAX_LEN,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0].shape)\n","print(attention_masks[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wz2PPAepSh-","colab_type":"code","colab":{}},"source":["batch_size = 16\n","\n","data_inputs_rest, validation_inputs, data_labels_rest, validation_labels = train_test_split(input_ids, labels, \n","                                                            random_state=123, test_size=0.1,shuffle=True)\n","data_masks_rest, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=123, test_size=0.1,shuffle=True)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgTwJ0WQpZ7s","colab_type":"code","colab":{}},"source":["print(validation_data[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"60BoKrikrWgO","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import KFold\n","splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=123).split(data_inputs_rest, data_labels_rest))\n","# kfold = KFold(n_splits=5,shuffle=True,random_state=123)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sSLy7SHbxUkd","colab_type":"code","colab":{}},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKXKlaTQxX2L","colab_type":"code","colab":{}},"source":["for fold, (train_idx, test_idx) in enumerate(splits):\n","    print(\"Training for fold {}\".format(fold))\n","    best_score = 0  \n","\n","    train_inputs = torch.tensor(data_inputs_rest[train_idx])\n","    test_inputs = torch.tensor(data_inputs_rest[test_idx])\n","    train_labels = torch.tensor(data_labels_rest[train_idx])\n","    test_labels = torch.tensor(data_labels_rest[test_idx])\n","    batch_size = 16\n","    train_masks = torch.tensor(data_masks_rest[train_idx])\n","    test_masks = torch.tensor(data_masks_rest[test_idx])\n","   \n","    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","   \n","\n","    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data , sampler=test_sampler , batch_size=batch_size)  \n","    model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-uncased\", num_labels=45)\n","    model.cuda()\n","    \n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'gamma', 'beta']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","        'weight_decay_rate': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","        'weight_decay_rate': 0.0}\n","    ]\n","    optimizer = BertAdam(optimizer_grouped_parameters,\n","                        lr=2e-5,\n","                        warmup=.1)\n","    t = [] \n","    train_loss_set = []\n","\n","    # Number of training epochs (authors recommend between 2 and 4)\n","    epochs = 10\n","\n","    for _ in trange(epochs, desc=\"Epoch\"):\n","      \n","      model.train()\n","      tr_loss = 0\n","      nb_tr_examples, nb_tr_steps = 0, 0\n","    \n","      for step, batch in enumerate(train_dataloader):\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        optimizer.zero_grad()\n","\n","        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","        train_loss_set.append(loss.item())    \n","        loss.backward()\n","        optimizer.step()\n","\n","        tr_loss += loss.item()\n","        nb_tr_examples += b_input_ids.size(0)\n","        nb_tr_steps += 1\n","\n","      print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","        \n","      model.eval()\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","\n","      # Evaluate data for one epoch\n","      for batch in validation_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","          logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","      print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","      model.eval()\n","\n","      predictions , true_labels = [], []\n","      for batch in test_dataloader:\n","\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","\n","          logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","      \n","        predictions.append(logits)\n","        true_labels.append(label_ids)\n","      from sklearn.metrics import matthews_corrcoef\n","      matthews_set = []\n","\n","      for i in range(len(true_labels)):\n","        matthews = matthews_corrcoef(true_labels[i],\n","                      np.argmax(predictions[i], axis=1).flatten())\n","        matthews_set.append(matthews)\n","      from sklearn.metrics import f1_score\n","      f1_score_set = []\n","      for i in range(len(true_labels)):\n","        f1_measure = f1_score(true_labels[i],\n","                      np.argmax(predictions[i], axis=1).flatten(), average='weighted')\n","        f1_score_set.append(f1_measure)\n","      flat_predictions = [item for sublist in predictions for item in sublist]\n","      flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","      flat_true_labels = [item for sublist in true_labels for item in sublist]\n","    \n","    print(\"f1_score\",f1_score(flat_true_labels, flat_predictions,average='weighted'))\n","    print(\"matthews_corrcoef\",matthews_corrcoef(flat_true_labels, flat_predictions))\n","    from sklearn.metrics import classification_report\n","    target_names = intents\n","    print(\"Precision, Recall and F1-Score:\\n\\n\",classification_report(flat_true_labels, flat_predictions, target_names=target_names))      \n","    filename='/content/drive/My Drive/Colab Notebooks/NLP/models/model_bertmultilingual_45_intents.pt'\n","    torch.save( model,filename)\n","    print(\"SAVED SUCESS\")\n"],"execution_count":null,"outputs":[]}]}